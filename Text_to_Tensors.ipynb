{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRYCGoKJNRpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae14a99-fb86-4fde-e97a-ebc688e54ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |                                | 10 kB 33.1 MB/s eta 0:00:01\r\u001b[K     |▏                               | 20 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |▎                               | 30 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |▎                               | 40 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |▍                               | 51 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |▌                               | 61 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |▋                               | 71 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 81 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |▊                               | 92 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 102 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 112 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 122 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 133 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 143 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 153 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 163 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 174 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 184 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 194 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 204 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 215 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 225 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 235 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 245 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 256 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 266 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 276 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 286 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 296 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 307 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 317 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 327 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 337 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 348 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 358 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 368 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 378 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 389 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 399 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 409 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 419 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 430 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 440 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 450 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 460 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 471 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 481 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 491 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 501 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 512 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 522 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 532 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 542 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 552 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 563 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 573 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 583 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 593 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 604 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 614 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 624 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 634 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 645 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 655 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 665 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 675 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 686 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 696 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 706 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 716 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 727 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 737 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 747 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 757 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 768 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 778 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 788 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 798 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 808 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 819 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 829 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 839 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 849 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 860 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 870 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 880 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 890 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 901 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 911 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 921 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 931 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 942 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 952 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 962 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 972 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 983 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 993 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 1.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 1.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 1.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 1.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 1.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 1.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 1.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 1.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 1.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 1.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 1.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 2.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 2.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 2.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 2.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 2.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 2.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 2.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 2.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 3.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 3.1 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 3.2 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 3.3 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 3.4 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 3.5 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 3.6 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 3.7 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 3.8 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 3.9 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 4.0 MB 29.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 4.0 MB 29.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification task\n",
        "In this module, we will start with a simple text classification task based on the AG_NEWS dataset: we'll classify news headlines into one of 4 categories: World, Sports, Business and Sci/Tech. To load the dataset, we will use the TensorFlow Datasets API."
      ],
      "metadata": {
        "id": "cafon-uSNlga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBn7Mk-eNVoj",
        "outputId": "71f21611-f99d-480b-8713-bd66b37fd177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GwST0y2M63RB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
        "# we will set tensorflow option to grow GPU memory allocation when required.\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "if len(physical_devices)>0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "dataset = pd.read_csv(\"BBC News Train.csv\")"
      ],
      "metadata": {
        "id": "zN77OqIhNydG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now access the training and test portions of the dataset by using dataset['train'] and dataset['test'] respectively:"
      ],
      "metadata": {
        "id": "DUUxxxFBOIVF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mR9f6_Cr6Wef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train =dataset.drop(\"Category\", axis = 1)\n",
        "ds_test = dataset[\"Category\"]\n",
        "\n",
        "print(f\"Length of train dataset = {len(ds_train)}\")\n",
        "print(f\"Length of test dataset = {len(ds_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB-8TMKlOLaQ",
        "outputId": "2412ab8f-9a7b-4d1b-ef64-098bcc0152f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset = 1490\n",
            "Length of test dataset = 1490\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = list(train[\"Category\"].unique())\n",
        "\n",
        "for i,x in zip(range(5),ds_train):\n",
        "    print(f\"{x['Text']} ({classes[x['Text']]}) -> {x['title']} {x['description']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "sljZQILdON9d",
        "outputId": "7cac68f0-a0c8-403b-efab-b656aeb0387d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3b81f55642e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ri9CyyIIOVWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text vectorization\n",
        "Now we need to convert text into numbers that can be represented as tensors. If we want word-level representation, we need to do two things:\n",
        "\n",
        "- Use a tokenizer to split text into tokens.\n",
        "- Build a vocabulary of those tokens.\n",
        "\n",
        "# Limiting vocabulary size\n",
        "In the AG News dataset example, the vocabulary size is rather big, more than 100k words. Generally speaking, we don't need words that are rarely present in the text — only a few sentences will have them, and the model will not learn from them. Thus, it makes sense to limit the vocabulary size to a smaller number by passing an argument to the vectorizer constructor:\n",
        "\n",
        "Both of those steps can be handled using the TextVectorization layer. Let's instantiate the vectorizer object, and then call the adapt method to go through all text and build a vocabulary:"
      ],
      "metadata": {
        "id": "4FJpEZ5uOlNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50000\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n",
        "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title']+' '+x['description']))"
      ],
      "metadata": {
        "id": "6QkhY9NjOoOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorizer.get_vocabulary()\n",
        "vocab_size = len(vocab)\n",
        "print(vocab[:10])\n",
        "print(f\"Length of vocabulary: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUnEn3TrO3kT",
        "outputId": "7fea1f0c-0be7-4dfd-cb26-c01d7be2027e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'to', 'a', 'in', 'of', 'and', 'on', 'for']\n",
            "Length of vocabulary: 5335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer('I love to play with my words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iV9WtF6O8Cz",
        "outputId": "cb6c19e6-ef68-405f-d892-0416433c53b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 112, 3695,    3,  304,   11, 1041,    1])>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-words text representation\n",
        "Because words represent meaning, sometimes we can figure out the meaning of a piece of text by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like weather and snow are likely to indicate weather forecast, while words like stocks and dollar would count towards financial news.\n",
        "\n",
        "Bag-of-words (BoW) vector representation is the most simple to understand traditional vector representation. Each word is linked to a vector index, and a vector element contains the number of occurrences of each word in a given document."
      ],
      "metadata": {
        "id": "WUeOQq-SPL4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sc_vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "sc_vectorizer.fit_transform(corpus)\n",
        "sc_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWZgD90YPFiB",
        "outputId": "775e9261-16f8-4e3b-c9be-50d1dec6ef6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_bow(text):\n",
        "    return tf.reduce_sum(tf.one_hot(vectorizer(text),vocab_size),axis=0)\n",
        "\n",
        "to_bow('My dog likes hot dogs on a hot day.').numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeTHAPzxPVo-",
        "outputId": "f887f686-9c65-410d-b475-757f7bd62a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 5., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the BoW classifier\n",
        "Now that we have learned how to build the bag-of-words representation of our text, let's train a classifier that uses it. First, we need to convert our dataset to a bag-of-words representation. This can be achieved by using map function in the following way:"
      ],
      "metadata": {
        "id": "ukoUCPr2PdQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "ds_train_bow = ds_train.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)\n",
        "ds_test_bow = ds_test.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)"
      ],
      "metadata": {
        "id": "12EQKEYYPaMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a simple classifier neural network that contains one linear layer. The input size is vocab_size, and the output size corresponds to the number of classes (4). Because we're solving a classification task, the final activation function is softmax:"
      ],
      "metadata": {
        "id": "RzFs2J2cPsJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(4,activation='softmax',input_shape=(vocab_size,))\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train_bow,validation_data=ds_test_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwyP4yz9Poa6",
        "outputId": "c39995e3-e706-4e59-8358-59baae830d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 59s 61ms/step - loss: 0.6154 - acc: 0.8416 - val_loss: 0.4416 - val_acc: 0.8705\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ed96d4f10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a classifier as one network\n",
        "Because the vectorizer is also a Keras layer, we can define a network that includes it, and train it end-to-end. This way we don't need to vectorize the dataset using map, we can just pass the original dataset to the input of the network."
      ],
      "metadata": {
        "id": "eyLtectmP0Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "inp = keras.Input(shape=(1,),dtype=tf.string)\n",
        "x = vectorizer(inp)\n",
        "x = tf.reduce_sum(tf.one_hot(x,vocab_size),axis=1)\n",
        "out = keras.layers.Dense(4,activation='softmax')(x)\n",
        "model = keras.models.Model(inp,out)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqoArVPGPxqh",
        "outputId": "b6d77661-3b24-489f-9e22-62855380be19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 5335)        0         \n",
            "                                                                 \n",
            " tf.math.reduce_sum (TFOpLam  (None, 5335)             0         \n",
            " bda)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 21344     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,344\n",
            "Trainable params: 21,344\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "938/938 [==============================] - 10s 10ms/step - loss: 0.5963 - acc: 0.8450 - val_loss: 0.4188 - val_acc: 0.8739\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ed8abc550>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigrams, trigrams and n-grams\n",
        "One limitation of the bag-of-words approach is that some words are part of multi-word expressions, for example, the word 'hot dog' has a completely different meaning from the words 'hot' and 'dog' in other contexts. If we represent the words 'hot' and 'dog' always using the same vectors, it can confuse our model.\n",
        "\n",
        "To address this, n-gram representations are often used in methods of document classification, where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. In bigram representations, for example, we will add all word pairs to the vocabulary, in addition to original words.\n",
        "\n",
        "Below is an example of how to generate a bigram bag of word representation using Scikit Learn:"
      ],
      "metadata": {
        "id": "3kGDC2PlQFtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "bigram_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
        "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnzuBobfQBO1",
        "outputId": "1de2c78d-ff38-4657-e02e-42e103308142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main drawback of the n-gram approach is that the vocabulary size starts to grow extremely fast. In practice, we need to combine the n-gram representation with a dimensionality reduction technique, such as embeddings, which we will discuss in the next unit.\n",
        "\n",
        "To use an n-gram representation in our AG News dataset, we need to pass the ngrams parameter to our TextVectorization constructor. The length of a bigram vocaculary is significantly larger, in our case it is more than 1.3 million tokens! Thus it makes sense to limit bigram tokens as well by some reasonable number.\n",
        "\n",
        "We could use the same code as above to train the classifier, however, it would be very memory-inefficient. In the next unit, we will train the bigram classifier using embeddings. In the meantime, you can experiment with bigram classifier training in this notebook and see if you can get higher accuracy."
      ],
      "metadata": {
        "id": "_OZUAt9CQis0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatically calculating BoW Vectors\n",
        "In the example above we calculated BoW vectors by hand by summing the one-hot encodings of individual words. However, the latest version of TensorFlow allows us to calculate BoW vectors automatically by passing the output_mode='count parameter to the vectorizer constructor. This makes defining and training our model significanly easier:"
      ],
      "metadata": {
        "id": "Cvic90GGQlpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='count'),\n",
        "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
        "])\n",
        "print(\"Training vectorizer\")\n",
        "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0QR9bvKQgeU",
        "outputId": "83e9ff73-b462-4b9e-ae7e-d947ce96c608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vectorizer\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.5910 - acc: 0.8499 - val_loss: 0.4163 - val_acc: 0.8757\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ed786f4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term frequency - inverse document frequency (TF-IDF)\n",
        "In BoW representation, word occurrences are weighted using the same technique regardless of the word itself. However, it's clear that frequent words such as a and in are much less important for classification than specialized terms. In most NLP tasks some words are more relevant than others.\n",
        "\n",
        "TF-IDF stands for term frequency - inverse document frequency. It's a variation of bag-of-words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the frequency of the word occurrence in the corpus."
      ],
      "metadata": {
        "id": "JXKYIL_UQzg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5vuhCsCQwss",
        "outputId": "aab5d234-cedb-4ba8-911f-dc6248206925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
              "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='tf-idf'),\n",
        "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
        "])\n",
        "print(\"Training vectorizer\")\n",
        "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sccT2d36S1pX",
        "outputId": "4eee3ab7-fc3f-450a-b2c1-9ba79cfff939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vectorizer\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.4179 - acc: 0.8670 - val_loss: 0.3412 - val_acc: 0.8876\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ed7015700>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "In our previous example, we operated on high-dimensional bag-of-words vectors with length vocab_size, and we explicitly converted low-dimensional positional representation vectors into sparse one-hot representation. This one-hot representation is not memory-efficient. In addition, each word is treated independently from each other, so one-hot encoded vectors don't express semantic similarities between words.\n",
        "\n",
        "In this unit, we will continue exploring the News AG dataset. To begin, let's load the data and get some definitions from the previous unit.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3CkifHGTP21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0\n",
        "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFVZ0zZtS6Ih",
        "outputId": "cad7ba07-e48e-423d-aef4-8f94510c27c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "batch_size = 128\n",
        "\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,input_shape=(1,))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,    \n",
        "    keras.layers.Embedding(vocab_size,100),\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNVZlmdCTxOr",
        "outputId": "1955524b-b3b2-4a00-852e-05cd6cf4cb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_3 (TextV  (None, None)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         3000000   \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 100)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 404       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,000,404\n",
            "Trainable params: 3,000,404\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "print(\"Training vectorizer\")\n",
        "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEhxkSCqXr2A",
        "outputId": "f363deaf-30a6-48ce-e222-162835f7cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training vectorizer\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.7953 - acc: 0.8087 - val_loss: 0.4522 - val_acc: 0.8624\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ed74ca6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with variable sequence sizes\n",
        "\n",
        "Let's understand how training happens in minibatches. In the example above, the input tensor has dimension 1, and we use 128-long minibatches, so that actual size of the tensor is $128 \\times 1$. However, the number of tokens in each sentence is different. If we apply the `TextVectorization` layer to a single input, the number of tokens returned is different, depending on how the text is tokenized:"
      ],
      "metadata": {
        "id": "IIa0r3iqY4P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer('Hello, world!'))\n",
        "print(vectorizer('I am glad to meet you!'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2BT-V8LX4d8",
        "outputId": "0a155593-1076-41a6-9357-13225bddef66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 1 45], shape=(2,), dtype=int64)\n",
            "tf.Tensor([ 112 1271    1    3 1747  158], shape=(6,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, when we apply the vectorizer to several sequences, it has to produce a tensor of rectangular shape, so it fills unused elements with the PAD token (which in our case is zero):"
      ],
      "metadata": {
        "id": "Ya_SdZyHahmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer(['Hello, world!','I am glad to meet you!'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhhoQsf4afZZ",
        "outputId": "339a0e6c-b2e4-4c97-eb6b-dfa35f9781ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 6), dtype=int64, numpy=\n",
              "array([[   1,   45,    0,    0,    0,    0],\n",
              "       [ 112, 1271,    1,    3, 1747,  158]])>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[1](vectorizer(['Hello, world!','I am glad to meet you!'])).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyN7XjBLaj9h",
        "outputId": "5106bfc2-7cc8-495e-a6e2-584a90f095e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.06041153,  0.02455312, -0.05121044, ...,  0.04758617,\n",
              "          0.05528081, -0.05168933],\n",
              "        [-0.01107537, -0.18788937, -0.27201608, ...,  0.25810242,\n",
              "          0.01245269, -0.17599559],\n",
              "        [-0.03603746,  0.00321708, -0.01367562, ..., -0.00999854,\n",
              "          0.04418064,  0.03704951],\n",
              "        [-0.03603746,  0.00321708, -0.01367562, ..., -0.00999854,\n",
              "          0.04418064,  0.03704951],\n",
              "        [-0.03603746,  0.00321708, -0.01367562, ..., -0.00999854,\n",
              "          0.04418064,  0.03704951],\n",
              "        [-0.03603746,  0.00321708, -0.01367562, ..., -0.00999854,\n",
              "          0.04418064,  0.03704951]],\n",
              "\n",
              "       [[ 0.15560809, -0.02057181, -0.3322794 , ...,  0.21971123,\n",
              "          0.04530514, -0.16191325],\n",
              "        [ 0.12922712,  0.0451119 , -0.17984207, ...,  0.11888123,\n",
              "          0.07744183, -0.08465374],\n",
              "        [ 0.06041153,  0.02455312, -0.05121044, ...,  0.04758617,\n",
              "          0.05528081, -0.05168933],\n",
              "        [ 0.05566093,  0.09769475,  0.08105124, ..., -0.08785198,\n",
              "         -0.02248834,  0.11849028],\n",
              "        [-0.21947989, -0.05830858,  0.08356667, ..., -0.13544594,\n",
              "          0.0966175 ,  0.12042065],\n",
              "        [ 0.24830072,  0.08285023, -0.41275895, ...,  0.26956138,\n",
              "         -0.05685811, -0.14722691]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXk3KREranYf",
        "outputId": "eb11c3fa-3fb0-43b0-c7d4-44faa5e8017e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('neural'):\n",
        "    print(f\"{w} -> {p}\")"
      ],
      "metadata": {
        "id": "TypBkFkbc7tV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v['play'][:20]"
      ],
      "metadata": {
        "id": "6rUumSFjc-SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king','woman'],negative=['man'])[0]"
      ],
      "metadata": {
        "id": "sOet4uf-dFHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the vector corresponding to kind-man+woman\n",
        "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
        "# find the index of the closest embedding vector \n",
        "d = np.sum((w2v.vectors-qvec)**2,axis=1)\n",
        "min_idx = np.argmin(d)\n",
        "# find the corresponding word\n",
        "w2v.index2word[min_idx]"
      ],
      "metadata": {
        "id": "cwOsRQQydMY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using pretrained embeddings in Keras\n",
        "\n",
        "We can modify the example above to prepopulate the matrix in our embedding layer with semantic embeddings, such as Word2Vec. The vocabularies of the pretrained embedding and the text corpus will likely not match, so we need to choose one. Here we explore the two possible options: using the tokenizer vocabulary, and using the vocabulary from Word2Vec embeddings.\n",
        "\n",
        "### Using tokenizer vocabulary\n",
        "\n",
        "When using the tokenizer vocabulary, some of the words from the vocabulary will have corresponding Word2Vec embeddings, and some will be missing. Given that our vocabulary size is `vocab_size`, and the Word2Vec embedding vector length is `embed_size`, the embedding layer will be repesented by a weight matrix of shape `vocab_size`$\\times$`embed_size`. We will populate this matrix by going through the vocabulary:"
      ],
      "metadata": {
        "id": "x5uPsiZSdylp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "W = np.zeros((vocab_size,embed_size))\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found = 0,0\n",
        "for i,w in enumerate(vocab):\n",
        "    try:\n",
        "        W[i] = w2v.get_vector(w)\n",
        "        found+=1\n",
        "    except:\n",
        "        # W[i] = np.random.normal(0.0,0.3,size=(embed_size,))\n",
        "        not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")"
      ],
      "metadata": {
        "id": "oB4COqzEdw5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = keras.layers.Embedding(vocab_size,embed_size,weights=[W],trainable=False)\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer, emb,\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "kALWIgWFd2FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
        "          validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "id": "HEdAZTRrd792"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using embedding vocabulary\n",
        "\n",
        "One issue with the previous approach is that the vocabularies used in the TextVectorization and Embedding are different. To overcome this problem, we can use one of the following solutions:\n",
        "* Re-train the Word2Vec model on our vocabulary.\n",
        "* Load our dataset with the vocabulary from the pretrained Word2Vec model. Vocabularies used to load the dataset can be specified during loading.\n",
        "\n",
        "The latter approach seems easier, so let's implement it. First of all, we will create a `TextVectorization` layer with the specified vocabulary, taken from the Word2Vec embeddings:"
      ],
      "metadata": {
        "id": "j9b-JX9TeKal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(w2v.vocab.keys())\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
        "vectorizer.set_vocabulary(vocab)"
      ],
      "metadata": {
        "id": "eSlIgQyWd_Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    vectorizer, \n",
        "    w2v.get_keras_embedding(train_embeddings=False),\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(128),validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
      ],
      "metadata": {
        "id": "SD_u7murePas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capture patterns with recurrent neural networks\n",
        "\n",
        "# Recurrent neural networks\n",
        "\n",
        "In the previous module, we covered rich semantic representations of text. The architecture we've been using captures the aggregated meaning of words in a sentence, but it does not take into account the **order** of the words, because the aggregation operation that follows the embeddings removes this information from the original text. Because these models are unable to represent word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.\n",
        "\n",
        "To capture the meaning of a text sequence, we'll use a neural network architecture called **recurrent neural network**, or RNN. When using an RNN, we pass our sentence through the network one token at a time, and the network produces some **state**, which we then pass to the network again with the next token.\n",
        "\n",
        "![Image showing an example recurrent neural network generation.](notebooks/images/rnn.png)\n",
        "\n",
        "Given the input sequence of tokens $X_0,\\dots,X_n$, the RNN creates a sequence of neural network blocks, and trains this sequence end-to-end using backpropagation. Each network block takes a pair $(X_i,S_i)$ as an input, and produces $S_{i+1}$ as a result. The final state $S_n$ or output $Y_n$ goes into a linear classifier to produce the result. All network blocks share the same weights, and are trained end-to-end using one backpropagation pass.\n",
        "\n",
        "> The figure above shows recurrent neural network in the unrolled form (on the left), and in more compact recurrent representation (on the right). It is important to realize that all RNN Cells have the same **shareable weights**.\n",
        "\n",
        "Because state vectors $S_0,\\dots,S_n$ are passed through the network, the RNN is able to learn sequential dependencies between words. For example, when the word *not* appears somewhere in the sequence, it can learn to negate certain elements within the state vector.\n",
        "\n",
        "Inside, each RNN cell contains two weight matrices: $W_H$ and $W_I$, and bias $b$. At each RNN step, given input $X_i$ and input state $S_i$, output state is calculated as $S_{i+1} = f(W_H\\times S_i + W_I\\times X_i+b)$, where $f$ is an activation function (often $\\tanh$).\n",
        "\n",
        "> For problems like text generation (that we will cover in the next unit) or machine translation we also want to get some output value at each RNN step. In this case, there is also another matrix $W_O$, and output is caluclated as $Y_i=f(W_O\\times S_i+b_O)$.\n",
        "\n",
        "Let's see how recurrent neural networks can help us classify our news dataset."
      ],
      "metadata": {
        "id": "6ZpOVF8begos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --quiet tensorflow_datasets==4.4.0\n",
        "!cd ~ && wget -q -O - https://mslearntensorflowlp.blob.core.windows.net/data/tfds-ag-news.tgz | tar xz"
      ],
      "metadata": {
        "id": "1zeMfMp_eVxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "# We are going to be training pretty large models. In order not to face errors, we need\n",
        "# to set tensorflow option to grow GPU memory allocation when required\n",
        "physical_devices = tf.config.list_physical_devices('GPU') \n",
        "if len(physical_devices)>0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()"
      ],
      "metadata": {
        "id": "9ddTnw88ey43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "embed_size = 64"
      ],
      "metadata": {
        "id": "e3v4RAFee1Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple RNN classifier\n",
        "\n",
        "In the case of a simple RNN, each recurrent unit is a simple linear network, which takes in an input vector and state vector, and produces a new state vector. In Keras, this can be represented by the `SimpleRNN` layer.\n",
        "\n",
        "While we can pass one-hot encoded tokens to the RNN layer directly, this is not a good idea because of their high dimensionality. Therefore, we will use an embedding layer to lower the dimensionality of word vectors, followed by an RNN layer, and finally a `Dense` classifier.\n",
        "\n",
        "> **Note**: In cases where the dimensionality isn't so high, for example when using character-level tokenization, it might make sense to pass one-hot encoded tokens directly into the RNN cell."
      ],
      "metadata": {
        "id": "kJSjxlA-e6Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    input_shape=(1,))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,\n",
        "    keras.layers.Embedding(vocab_size, embed_size),\n",
        "    keras.layers.SimpleRNN(16),\n",
        "    keras.layers.Dense(4,activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eygNI1V3e9HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train our RNN. RNNs in general are quite difficult to train, because once the RNN cells are unrolled along the sequence length, the resulting number of layers involved in backpropagation is quite large. Thus we need to select a smaller learning rate, and train the network on a larger dataset to produce good results. This can take quite a long time, so using a GPU is preferred.\n",
        "\n",
        "To speed things up, we will only train the RNN model on news titles, omitting the description. You can try training with description and see if you can get the model to train."
      ],
      "metadata": {
        "id": "yN6f19mpfDSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_title(x):\n",
        "    return x['title']\n",
        "\n",
        "def tupelize_title(x):\n",
        "    return (extract_title(x),x['label'])\n",
        "\n",
        "print('Training vectorizer')\n",
        "vectorizer.adapt(ds_train.take(2000).map(extract_title))"
      ],
      "metadata": {
        "id": "vyByYmw8e_zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
        "model.fit(ds_train.map(tupelize_title).batch(batch_size),validation_data=ds_test.map(tupelize_title).batch(batch_size))"
      ],
      "metadata": {
        "id": "wXy5yCiGfGBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Revisiting variable sequences \n",
        "\n",
        "Remember that the `TextVectorization` layer will automatically pad sequences of variable length in a minibatch with pad tokens. It turns out that those tokens also take part in training, and they can complicate convergence of the model.\n",
        "\n",
        "There are several approaches we can take to minimize the amount of padding. One of them is to reorder the dataset by sequence length and group all sequences by size. This can be done using the `tf.data.experimental.bucket_by_sequence_length` function (see [documentation](https://www.tensorflow.org/api_docs/python/tf/data/experimental/bucket_by_sequence_length)). \n",
        "\n",
        "Another approach is to use **masking**. In Keras, some layers support additional input that shows which tokens should be taken into account when training. To incorporate masking into our model, we can either include a separate `Masking` layer ([docs](https://keras.io/api/layers/core_layers/masking/)), or we can specify the `mask_zero=True` parameter of our `Embedding` layer.\n",
        "\n",
        "> **Note**: This training will take around 5 minutes to complete one epoch on the whole dataset. Feel free to interrupt training at any time if you run out of patience. What you can also do is limit the amount of data used for training, by adding `.take(...)` clause after `ds_train` and `ds_test` datasets."
      ],
      "metadata": {
        "id": "utnJqugqftVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,\n",
        "    keras.layers.Embedding(vocab_size,embed_size,mask_zero=True),\n",
        "    keras.layers.SimpleRNN(16),\n",
        "    keras.layers.Dense(4,activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "P8mYktvCfK8m",
        "outputId": "b848df62-2ac5-4f9a-c9fb-c5ab49fdbfc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5290be060dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model = keras.models.Sequential([\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5m0u5rS3f8ya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}